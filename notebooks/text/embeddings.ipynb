{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Примеры работы с векторными представлениями слов с использованием word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для экспериментов будем использовать библиотеку [gensim.](https://radimrehurek.com/gensim/auto_examples/index.html#documentation)\n",
    "\n",
    "Используем предобученную модель с сайта [RusVectores.](https://rusvectores.org/ru/models/)\n",
    "\n",
    "Необходимо отметить, что в существующих универсальных библиотеках (spacy, natasha) также реализованы возможности для работы с векторными представлениями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'corpus/ruscorpora_mystem_cbow_300_2_2015.bin.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выдаем ближайшие слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['король_S', 'королева_S', 'мужчина_S', 'женщина_S']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "СЛОВО - король_S\n",
      "5 ближайших соседей слова:\n",
      "принц_S => 0.672407865524292\n",
      "герцог_S => 0.6092391610145569\n",
      "королева_S => 0.5848619937896729\n",
      "император_S => 0.5664135813713074\n",
      "курфюрст_S => 0.5498137474060059\n",
      "\n",
      "СЛОВО - королева_S\n",
      "5 ближайших соседей слова:\n",
      "король_S => 0.5848619937896729\n",
      "принцесса_S => 0.5745126008987427\n",
      "герцогиня_S => 0.5264037847518921\n",
      "принц_S => 0.4866287410259247\n",
      "герцог_S => 0.4809737503528595\n",
      "\n",
      "СЛОВО - мужчина_S\n",
      "5 ближайших соседей слова:\n",
      "женщина_S => 0.7638137340545654\n",
      "девушка_S => 0.6010492444038391\n",
      "вамп_A => 0.571452260017395\n",
      "девица_S => 0.5431127548217773\n",
      "немолодой_A => 0.5392476320266724\n",
      "\n",
      "СЛОВО - женщина_S\n",
      "5 ближайших соседей слова:\n",
      "девушка_S => 0.7725452184677124\n",
      "мужчина_S => 0.7638137340545654\n",
      "девица_S => 0.6532836556434631\n",
      "дама_S => 0.6249092817306519\n",
      "человек_S => 0.5979774594306946\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    if word in model:\n",
    "        print('\\nСЛОВО - {}'.format(word))\n",
    "        print('5 ближайших соседей слова:')\n",
    "        for word, sim in model.most_similar(positive=[word], topn=5):\n",
    "            print('{} => {}'.format(word, sim))\n",
    "    else:\n",
    "        print('Слово \"{}\" не найдено в модели'.format(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Находим близость между словами и строим аналогии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.584862\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('король_S', 'королева_S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76381373\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('мужчина_S', 'женщина_S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('принц_S', 0.5996884107589722), ('герцог_S', 0.5775512456893921), ('королева_S', 0.5213630199432373), ('принцесса_S', 0.5140257477760315), ('герцогиня_S', 0.5112662315368652), ('император_S', 0.5007482171058655), ('посланник_S', 0.4788142442703247), ('посол_S', 0.453449547290802), ('царь_S', 0.4492655396461487), ('императрица_S', 0.4435415267944336)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=['король_S', 'женщина_S'], negative=['мужчина_S']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4549625\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('франция_S', 'париж_S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39409316\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('испания_S', 'мадрид_S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('лондон_S', 0.6315737366676331), ('берлин_S', 0.6051103472709656), ('мюнхен_S', 0.5779414176940918), ('нью-йорк_S', 0.5686920881271362), ('москва_S', 0.5660902261734009), ('амстердам_S', 0.5587795376777649), ('милан_S', 0.5564035773277283), ('венеция_S', 0.5518475770950317), ('авиньон_S', 0.5482670068740845), ('копенгаген_S', 0.539797842502594)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=['париж_S', 'испания_S'], negative=['франция_S']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучим word2vec на наборе данных imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Paladin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best scene in the movie was when Gerardo i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  value\n",
       "0  A very, very, very slow-moving, aimless movie ...      0\n",
       "1  Not sure who was more lost - the flat characte...      0\n",
       "2  Attempting artiness with black & white and cle...      0\n",
       "3       Very little music or anything to speak of.        0\n",
       "4  The best scene in the movie was when Gerardo i...      1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузка данных\n",
    "imdb_df = pd.read_csv(\"data/imdb_labelled.txt\", delimiter='\\t', header=None, names=['text', 'value'])\n",
    "imdb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовим корпус\n",
    "corpus = []\n",
    "stop_words = stopwords.words('english')\n",
    "tok = WordPunctTokenizer()\n",
    "for line in imdb_df['text'].values:\n",
    "    line1 = line.strip().lower()\n",
    "    line1 = re.sub(\"[^a-zA-Z]\",\" \", line1)\n",
    "    text_tok = tok.tokenize(line1)\n",
    "    text_tok1 = [w for w in text_tok if not w in stop_words]\n",
    "    corpus.append(text_tok1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['slow',\n",
       "  'moving',\n",
       "  'aimless',\n",
       "  'movie',\n",
       "  'distressed',\n",
       "  'drifting',\n",
       "  'young',\n",
       "  'man'],\n",
       " ['sure',\n",
       "  'lost',\n",
       "  'flat',\n",
       "  'characters',\n",
       "  'audience',\n",
       "  'nearly',\n",
       "  'half',\n",
       "  'walked'],\n",
       " ['attempting',\n",
       "  'artiness',\n",
       "  'black',\n",
       "  'white',\n",
       "  'clever',\n",
       "  'camera',\n",
       "  'angles',\n",
       "  'movie',\n",
       "  'disappointed',\n",
       "  'became',\n",
       "  'even',\n",
       "  'ridiculous',\n",
       "  'acting',\n",
       "  'poor',\n",
       "  'plot',\n",
       "  'lines',\n",
       "  'almost',\n",
       "  'non',\n",
       "  'existent'],\n",
       " ['little', 'music', 'anything', 'speak'],\n",
       " ['best',\n",
       "  'scene',\n",
       "  'movie',\n",
       "  'gerardo',\n",
       "  'trying',\n",
       "  'find',\n",
       "  'song',\n",
       "  'keeps',\n",
       "  'running',\n",
       "  'head']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# количество текстов в корпусе не изменилось и соответствует целевому признаку\n",
    "assert imdb_df.shape[0]==len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 41.3 ms\n"
     ]
    }
   ],
   "source": [
    "%time model_imdb = word2vec.Word2Vec(corpus, workers=4, min_count=10, window=10, sample=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ever', 0.34744006395339966), ('watching', 0.31204521656036377), ('predictable', 0.28830358386039734), ('acting', 0.2815362811088562), ('really', 0.27776145935058594)]\n"
     ]
    }
   ],
   "source": [
    "# Проверим, что модель обучилась\n",
    "print(model_imdb.wv.most_similar(positive=['find'], topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Решение задачи анализа тональности текста на основе модели word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(v, c):\n",
    "    model = Pipeline(\n",
    "        [(\"vectorizer\", v), \n",
    "         (\"classifier\", c)])\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print_accuracy_score_for_classes(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingVectorizer(object):\n",
    "    '''\n",
    "    Для текста усредним вектора входящих в него слов\n",
    "    '''\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.size = model.vector_size\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([np.mean(\n",
    "            [self.model[w] for w in words if w in self.model] \n",
    "            or [np.zeros(self.size)], axis=0)\n",
    "            for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score_for_classes(\n",
    "    y_true: np.ndarray, \n",
    "    y_pred: np.ndarray) -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    Вычисление метрики accuracy для каждого класса\n",
    "    y_true - истинные значения классов\n",
    "    y_pred - предсказанные значения классов\n",
    "    Возвращает словарь: ключ - метка класса, \n",
    "    значение - Accuracy для данного класса\n",
    "    \"\"\"\n",
    "    # Для удобства фильтрации сформируем Pandas DataFrame \n",
    "    d = {'t': y_true, 'p': y_pred}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    # Метки классов\n",
    "    classes = np.unique(y_true)\n",
    "    # Результирующий словарь\n",
    "    res = dict()\n",
    "    # Перебор меток классов\n",
    "    for c in classes:\n",
    "        # отфильтруем данные, которые соответствуют \n",
    "        # текущей метке класса в истинных значениях\n",
    "        temp_data_flt = df[df['t']==c]\n",
    "        # расчет accuracy для заданной метки класса\n",
    "        temp_acc = accuracy_score(\n",
    "            temp_data_flt['t'].values, \n",
    "            temp_data_flt['p'].values)\n",
    "        # сохранение результата в словарь\n",
    "        res[c] = temp_acc\n",
    "    return res\n",
    "\n",
    "def print_accuracy_score_for_classes(\n",
    "    y_true: np.ndarray, \n",
    "    y_pred: np.ndarray):\n",
    "    \"\"\"\n",
    "    Вывод метрики accuracy для каждого класса\n",
    "    \"\"\"\n",
    "    accs = accuracy_score_for_classes(y_true, y_pred)\n",
    "    if len(accs)>0:\n",
    "        print('Метка \\t Accuracy')\n",
    "    for i in accs:\n",
    "        print('{} \\t {}'.format(i, accs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучающая и тестовая выборки\n",
    "boundary = 700\n",
    "X_train = corpus[:boundary] \n",
    "X_test = corpus[boundary:]\n",
    "y_train = imdb_df.value.values[:boundary]\n",
    "y_test = imdb_df.value.values[boundary:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t Accuracy\n",
      "0 \t 0.46153846153846156\n",
      "1 \t 0.6\n"
     ]
    }
   ],
   "source": [
    "sentiment(EmbeddingVectorizer(model_imdb.wv), LogisticRegression(C=5.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Примеры работы с векторными представлениями слов с использованием fastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- На сайте проекта можно найти предобученные модели более чем для 150 языков (в том числе для русского) - https://fasttext.cc/docs/en/crawl-vectors.html Модель занимает около 4 Гб.\n",
    "- Будем использовать архив \"cc.ru.300.bin.gz\", из которого необходимо развернуть файл \"cc.ru.300.bin\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_2 = 'corpus/cc.ru.300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "ft = fasttext.load_model(model_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.27075031e-01, -2.77306233e-02,  3.26351188e-02,  5.07293902e-02,\n",
       "       -3.48336250e-03,  1.06337778e-02,  5.24145477e-02,  4.64887954e-02,\n",
       "       -1.85684413e-02, -5.92509583e-02, -2.40875334e-02,  6.52384833e-02,\n",
       "       -6.83054626e-02,  2.44189240e-02, -3.31012905e-02, -6.06933013e-02,\n",
       "        3.57930176e-02,  9.51208398e-02,  3.62336054e-03,  4.90398891e-02,\n",
       "        9.04162787e-03, -1.33480725e-03, -8.91964361e-02,  8.37044716e-02,\n",
       "        1.07321146e-04,  4.47255634e-02,  5.13922237e-02,  4.88838479e-02,\n",
       "        2.32052188e-02,  4.61901017e-02,  3.05132102e-02, -3.12336516e-02,\n",
       "        6.90741241e-02, -1.42289407e-03,  6.80347672e-03, -6.30118325e-02,\n",
       "        1.08894575e-02, -9.17965546e-02, -3.11685856e-02, -7.58677945e-02,\n",
       "        4.10646088e-02, -3.64330709e-02, -4.30045091e-02,  1.04629165e-02,\n",
       "        3.18807773e-02,  8.71223733e-02, -1.74481511e-01,  2.31932383e-02,\n",
       "        4.89283912e-02,  7.86454380e-02,  3.05795725e-02,  2.92000901e-02,\n",
       "        4.17612605e-02,  8.58819634e-02, -1.60420060e-01,  3.68188019e-03,\n",
       "        7.86922276e-02, -8.48372132e-02,  1.14311157e-02,  8.93299952e-02,\n",
       "        3.85963172e-03,  5.79128377e-02, -1.07770495e-01, -4.52252403e-02,\n",
       "        1.45375878e-02,  5.81156388e-02,  8.05756301e-02, -3.11780851e-02,\n",
       "       -6.17446899e-02,  1.98192876e-02, -7.56517574e-02,  3.30408588e-02,\n",
       "       -1.01617156e-02, -2.86370348e-02,  1.05185909e-02,  2.86398884e-02,\n",
       "       -1.45053240e-02, -8.38710591e-02, -4.09691669e-02, -6.12175604e-03,\n",
       "       -1.10071264e-01,  3.03787831e-02, -1.16728013e-02,  6.30235067e-03,\n",
       "       -3.24303075e-03,  3.92952338e-02, -9.04475972e-02,  7.66201541e-02,\n",
       "        8.11704155e-03,  4.30945866e-03,  9.70086921e-03, -6.66507334e-02,\n",
       "        4.58706915e-03, -1.31048560e-02, -2.90410910e-02, -2.44614407e-02,\n",
       "       -3.48833203e-02, -2.63981074e-02,  1.95412394e-02,  1.94606241e-02,\n",
       "       -1.23833120e-01,  4.78546694e-02,  2.93412339e-02, -1.83941554e-02,\n",
       "        3.57037270e-03, -1.56384092e-02,  5.02187982e-02,  9.63407531e-02,\n",
       "       -7.45296702e-02, -1.83583163e-02, -4.40289490e-02, -3.49901691e-02,\n",
       "        3.20321321e-02,  6.96790144e-02,  2.96200602e-03,  1.86711233e-02,\n",
       "        6.46503344e-02, -5.34856343e-04, -4.02927846e-02,  1.10604020e-03,\n",
       "       -3.69113423e-02, -4.70175073e-02, -7.89018199e-02,  2.66588833e-02,\n",
       "        7.20349476e-02,  5.82243465e-02, -1.95067991e-02, -5.56394160e-02,\n",
       "        6.43958673e-02,  3.80677991e-02,  8.95360578e-03, -6.31098077e-02,\n",
       "        2.01119930e-02, -1.13759212e-01, -8.25053155e-02, -2.13480797e-02,\n",
       "        7.97310397e-02,  9.22046080e-02,  5.69374561e-02, -2.58865803e-02,\n",
       "       -5.92980348e-02,  3.42016444e-02, -1.06792897e-03, -1.77407861e-02,\n",
       "       -2.96746250e-02, -5.17588221e-02,  3.76014300e-02, -8.57410114e-03,\n",
       "        1.07521079e-01,  2.17828490e-02,  4.45751958e-02, -2.24473383e-02,\n",
       "       -7.29674548e-02,  6.54779375e-02,  4.83158864e-02,  5.67849353e-03,\n",
       "       -7.97531044e-04,  9.78819188e-03,  3.73100676e-02, -7.87231922e-02,\n",
       "       -1.83365792e-02,  5.06487070e-03, -1.90280274e-01,  8.66564643e-03,\n",
       "       -3.21253873e-02, -2.44976785e-02, -1.99692752e-02,  5.42505272e-02,\n",
       "        3.43043171e-02,  5.19713480e-03,  3.51521350e-03,  2.78871488e-02,\n",
       "        1.18058326e-03,  2.76132650e-03, -8.07785429e-03,  5.10430932e-02,\n",
       "       -1.68284848e-02, -8.66510496e-02,  5.98207116e-03, -2.79562175e-03,\n",
       "       -7.14462856e-03,  4.16922569e-02,  5.75278513e-02, -6.09842269e-03,\n",
       "       -2.34537777e-02,  3.23416851e-02, -2.49151718e-02, -4.91601601e-02,\n",
       "        3.59974317e-02,  6.71616793e-02, -9.18544903e-02,  5.19581921e-02,\n",
       "       -5.89743666e-02,  4.37970124e-02,  8.74125883e-02,  7.24141002e-02,\n",
       "        5.49416542e-02,  4.62772027e-02,  2.12693326e-02,  7.50373825e-02,\n",
       "        7.33868033e-02,  4.12044674e-03, -3.42800133e-02,  6.30429909e-02,\n",
       "        5.90246730e-02, -9.45869461e-02,  3.04833986e-02, -5.78084961e-04,\n",
       "        4.58811671e-02,  4.86042872e-02,  1.04986569e-02, -1.78546961e-02,\n",
       "        8.33110437e-02, -3.12142558e-02,  4.05338369e-02, -5.04052304e-02,\n",
       "        3.82924074e-04,  3.28271799e-02,  7.26947263e-02,  4.04691957e-02,\n",
       "        8.18277057e-03, -2.17595901e-02,  3.27710062e-03, -9.59422365e-02,\n",
       "        5.01750596e-02, -4.81783524e-02, -1.07251443e-01,  4.32455130e-02,\n",
       "       -1.99180562e-02,  6.55251369e-02, -2.69376691e-02, -2.90191770e-02,\n",
       "        3.95500921e-02,  7.77703535e-04,  4.80472259e-02, -5.94006777e-02,\n",
       "       -1.03654169e-01,  4.96198311e-02, -1.52933151e-02,  1.16200529e-01,\n",
       "        1.19259255e-02, -2.03546584e-02, -7.08933249e-02, -1.16646830e-02,\n",
       "       -5.55511601e-02,  1.38687855e-02, -3.18548791e-02, -2.64572930e-02,\n",
       "       -1.20711466e-02, -8.98918230e-03,  2.07081493e-02,  5.06098941e-02,\n",
       "       -8.17159936e-02, -1.89090911e-02, -3.32116559e-02,  7.51760900e-02,\n",
       "       -6.07905164e-02, -1.03467278e-01, -1.58751920e-01,  1.06064295e-02,\n",
       "       -8.64740983e-02,  1.01918576e-03, -8.40022638e-02,  3.58039699e-02,\n",
       "       -4.25682031e-02, -1.24329686e-01,  6.96239481e-03,  3.37698832e-02,\n",
       "        6.05425350e-02, -7.11968020e-02,  9.78638083e-02,  4.13557850e-02,\n",
       "        7.50389323e-02, -3.12502240e-03, -9.85175092e-03,  5.79876900e-02,\n",
       "       -9.27843247e-03, -3.09136510e-02, -2.13580187e-02, -1.09149396e-01,\n",
       "       -1.29656736e-02,  2.11587362e-03,  2.52602398e-02,  5.93545800e-03,\n",
       "        7.27704689e-02, -6.65039718e-02,  3.39620411e-02,  8.12500622e-03,\n",
       "       -2.23514959e-02,  2.90179439e-02, -9.92280524e-03, -1.01059116e-01,\n",
       "        9.67219844e-02, -5.20229004e-02,  1.16604408e-02,  8.60411152e-02,\n",
       "        4.11232822e-02, -1.12516331e-02,  5.96832894e-02,  1.30593255e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Векторное представление слова \"вектор\"\n",
    "ft['вектор']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Находим близость между словами и строим аналогии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.7491326332092285, 'Санкт-Петербург'),\n",
       " (0.6996231079101562, '.Москва'),\n",
       " (0.6727142333984375, 'Россия'),\n",
       " (0.6636898517608643, 'г.Москва'),\n",
       " (0.6515657901763916, 'Москва.'),\n",
       " (0.6510270833969116, 'Новосибирск'),\n",
       " (0.6483853459358215, '-Москва'),\n",
       " (0.6445652842521667, 'Екатеринбург'),\n",
       " (0.6321169137954712, 'Московская'),\n",
       " (0.6303269863128662, 'Г.Москва')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ближайшие соседи\n",
    "ft.get_nearest_neighbors('Москва')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.8113399147987366, 'королева'),\n",
       " (0.7215961217880249, 'королева-мать'),\n",
       " (0.6778442859649658, 'Король'),\n",
       " (0.666719377040863, 'принцесса'),\n",
       " (0.6663369536399841, 'Королева-мать'),\n",
       " (0.6476640105247498, 'королева-регентша'),\n",
       " (0.6474599242210388, 'правительница'),\n",
       " (0.636768639087677, 'царица'),\n",
       " (0.6327172517776489, 'королевская'),\n",
       " (0.6122475266456604, 'короля')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# аналогии\n",
    "ft.get_analogies('женщина', 'мужчина', 'король')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.8256334662437439, 'опечатка'),\n",
       " (0.7491827607154846, 'Очепятка'),\n",
       " (0.7455087304115295, 'ошибочка'),\n",
       " (0.7255198955535889, 'описка'),\n",
       " (0.7242795825004578, 'очепятку'),\n",
       " (0.7071713805198669, 'Опечатка'),\n",
       " (0.6669175624847412, 'очепятки'),\n",
       " (0.6004636883735657, 'опечаточка'),\n",
       " (0.5953572988510132, 'Ошибочка'),\n",
       " (0.5925487279891968, 'опечатался')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Возможно использование слов, которых не было в исходном словаре, \n",
    "# в том числе слов с опечатками\n",
    "ft.get_nearest_neighbors('очепятка')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
